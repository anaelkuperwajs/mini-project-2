---
title: "Mini-Project 2: Adventure 1"
author: Anael Kuperwajs Cohen, Juliet Kelson
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---


```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(stringr)
library(tidyverse)
library(quanteda)
library(syuzhet)
source("miniProjFunctions.R")
```


\
\



## Part 1: Process the data

```{r}
buzzfeed <- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv")

buzzfeed <- buzzfeed %>% 
  mutate(title = as.character(title),
         text = as.character(text),
         url = as.character(url))
```



### New predictors

Our new predictors will be:

[x] 1. Word count
[x] 2. Word count in title
[x] 3. Upper-case word count
[x] 4. Upper-case word count in title
[x] 5. Upper-case word ratio in title
[x] 6. ! ratio (to . ?)
[x] 7. ! ratio (to . ?) in title
[x] 8. Sentence Count
[x] 9. Syllables
[x] 10. % Unique words
[x] 11. Flesch–Kincaid grade level
[x] 12. Author Count
[x] 13. Primary sentiment
[x] 14. Secondary sentiment
[x] 15. Strength of primary sentiment
[x] 16. Strength of secondary sentiment


1. Word Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words = str_count(text, " ") + 1)
```

2. Word count in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words_title = str_count(title, " ") + 1)
```

3. Upper-case word count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words = str_count(text, "\\b[A-Z]{2,}\\b"))
```

4. Upper-case word count in title
5. Upper-case word ratio in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words_title = str_count(title, "\\b[A-Z]{2,}\\b"),
         upper_case_word_ratio_title = total_upper_case_words_title/total_words_title)
```

6. ! ratio (to . ?)
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio = exclamation_ratio(text))
```

7. ! ratio (to . ?) in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio_title = exclamation_ratio(title))
```

8. Sentence Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_sentences = nsentence(text))
```

9. Syllables
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_syllables = nsyllable(text))
```

10. % Unique words
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(unique_word_percent = pct_unique_words(text, total_words))
```

11. Flesch–Kincaid grade level
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(readability_score = flesch_reading_ease(total_words, total_sentences, total_syllables))
```

12. Author Count
```{r}
buzzfeed <- buzzfeed %>%
  mutate(authors = gsub("View All Posts,", "", authors)) %>%
  mutate(authors = gsub("View All Posts", "", authors)) %>%
  mutate(authors = gsub("Abc News,", "", authors)) %>%
  mutate(authors = gsub("Abc News", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter,", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent,", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent", "", authors)) %>%
  mutate(authors = gsub("Latest Posts,", "", authors)) %>%
  mutate(authors = gsub("Latest Posts", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer", "", authors)) %>%
  mutate(author_count = str_count(authors, ",") + 1) %>% 
  mutate(author_count = if_else(author_count == 0, 1, author_count)) %>% 
  mutate(authors = if_else(authors == "", "None", authors)) %>% 
  mutate(author_count = if_else(authors == "None", 0, author_count))
```

13. Primary sentiment
14. Secondary sentiment
15. Strength of primary sentiment
16. Strength of secondary sentiment
```{r cache=TRUE}
buzzfeed <- buzzfeed %>% 
  rowwise() %>% 
  mutate(primary_sentiment = max_sentiment_type(text, 1),
         primary_sentiment_value = max_sentiment_value(text, 1),
         secondary_sentiment = max_sentiment_type(text, 2),
         secondary_sentiment_value = max_sentiment_value(text, 2)
         )
```

```{r}
buzzfeed[c(8,153),]
```

The two sources that will be presented in order to demonstrate our new predictors include "Jeb Bush to lecture at Harvard this fall" -- the real sample article -- and "Hillary's DEAD!?!? Brand New Theory Has Serious PROOF" --  the fake article. 
We first started by looking at predictors relating to word and punctuation counts in the text itself and the title. Our first predictors looked at the word count and upper-case word count in the text and the title. The total number of words for the title and the text were not significantly different between these two articles. The number of words in the title is exactly the same and the word count in the text is greater for the fake article, but only by 50 words. The total number of upper-case words in the text is also similar in both articles, one in the real article and two in the fake, however the difference in the number of upper-case words is more drastic in the title. There are zero upper-case words in the real title and two in the fake article, which is out of eight total words in the title. Two out of eight words is 25%, as seen in the ratio predictor for upper-case words in the title. The next two predictors looked at the ratio of exclamation points to question marks and periods in the text and title. In the text, there are zero exclamation points in real article, yet about 47% in the fake article. Furthermore, in the title there are zero exclamation points in the real article but they make up 50% of the punctuation in the fake article. This shows that exclamation points are used way more freely in the fake article, rather than the real article. Just like word count, we also looked at sentence and syllable count. These two measurements were fairly insignificant as the numbers were similar in both sentences. It was slightly surprising, however, that there were more sentences and syllables in the fake news article than the real one. The next two predictors analyzed the words that were used in the text. The first is the percent of unique words, which was much higher for the real article, 112.75% vs 87.10%. The second is a readability score, where the higher the percent the easier the text is to read because it is meant for a younger audience. The real article has a readability score of 23.34, which matches with a school reading level of a college graduate. On the other hand, the fake article has a score of 48.71, which aligns with a school reading level of a college student. While this may not seem like a huge difference, the idea is that the structure of the article is much more complex the lower the score. Fake articles often don't have that same complexity. The next predictor is about the author count, which is equal in this case between the two articles with one author. It does not make a big difference in this comparison. Finally, the last four predictors are about sentiment analysis. We looked at the primary and secondary sentiment in the text and the strength of that sentiment. The primary sentiment is the strongest sentiment in the text and the secondy sentiment is the one with the second highest score. The sentiments for the real article are trust and positive, with scores in the 30s, while the fake article has the sentiments negative and positive with low scores of 16 each. The fact that the primary and secondary sentiments conflict with each other greatly is something to be concerned about when thinking of the validity of an article.

Similar to any analytical method, text analysis has it's drawbacks. One drawback, for example, is seen clearly through the sentiment analysis predictors. We will not always be able to accurately analyze the sentiment of text. Computers and algorithms can't always detect sarcasm, understand references or allusions, and won't always correctly analyze basic sentiments such as `positive` or `negative`. Additionally, writing algorithms to detect factors like writing style, words with multiple meanings, correct grammar/puntuation, and correctness of fact is often difficult, innacurate, or impossible.  All of that being said, text analysis is still a beneficial and important research tool.


\
\
\
\
\
\



## Part 2: Analyze


LASSO with source
```{r}
set.seed(253)
lambda_grid <- 10^seq(-3, 1, length = 100)

lasso_model <- train(
    type ~ .,
    data = buzzfeed %>% select(-text, -title, -url),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "Accuracy",
    na.action = na.omit
)

lasso_model$results %>% filter(lambda == lasso_model$bestTune$lambda)
coef(lasso_model$finalModel, lasso_model$bestTune$lambda)


predict_data <- na.omit(lasso_model$trainingData)
classifications <- predict(lasso_model, newdata = predict_data, type = "raw")
head(classifications, 3)


confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```

LASSO without source
```{r}
set.seed(253)
lambda_grid <- 10^seq(-3, 1, length = 100)

lasso_model_no_source <- train(
    type ~ .,
    data = buzzfeed %>% select(-text, -title, -url, -source),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "Accuracy",
    na.action = na.omit
)

lasso_model_no_source$results %>% filter(lambda == lasso_model_no_source$bestTune$lambda)
coef(lasso_model_no_source$finalModel, lasso_model_no_source$bestTune$lambda)


predict_data_no_source <- na.omit(lasso_model_no_source$trainingData)
classifications <- predict(lasso_model_no_source, newdata = predict_data, type = "raw")
head(classifications, 3)


confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```

To build our model, we first looked at the relationships between predictors and types of article. We then tried a variety of techniques to get a sense of which might do the best job of predicting if the articles are real or fake.  We thought about KNN and GAM, but GAM did not do well with so much text and KNN does not do so well with so many predictors, so we moved on to parametric algorithms.

We chose LASSO with binary classification because simple logistic regression tried to use all of the predictors with no penalties and did not do as good of a job making accurate predictions. We did not try backwards stepwise or best subset selection because they are more computationally expensive.  LASSO had a very high accuracy and high specificity so we moved forward with LASSO.

From here we made two models -- one that includes `source` and one that does not. We did this because source significantly affects accuracy.  If we were to try classifying an article with no source, the accuracy would be lower and the model would be different. When training our model, we excluded `text`, `title`, and `url` from the data because they are unique to each article and therefore do not make useful predictors.

For our models, we used a broad range of lambda values to get a better understanding of what size of penalty yielded the highest accuracy.  We chose `best`, because for a topic like fake news with potentialy impactful consequences, it is important to be as accurate as possible.

```{r}
plot(lasso_model, xlim=c(0, .3))
plot(lasso_model_no_source, xlim=c(0, .3))
```


\
\
\
\
\
\



## Part 3: Summarize

After analyzing our data and looking at the results, we can conclude that some of the best predictors for our model included source, exclamation ratio in the text and title, authors, and surprise sentiment. Some of the worst predictors included the four sentiment predictors: primary and secondary sentiment and sentiment strength. The predictors that did not lean in either direction include `readability_score`, `unique_word_percent`, and `author_count` because they weren't disjoint and there was some overlap but not complete overlap between real and fake articles. We did, however, have two different models, one with source and one without source as a predictor. Both are very accurate, but when taking source out as a predictor, sentiment becomes a better predictor.

```{r}
buzzfeed %>% 
  ggplot(aes(x = source, fill= type))+
  geom_bar()+ 
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(breaks = scales::pretty_breaks(10))+
  labs(x="Source", y="Count",  fill= "Type", title="Number of Real vs Fake articles by source")
```

We can see from the plot above that there is very little overlap of real and fake news for each source. This makes it an extremely good predictor.

We can also see that the exclamation ratios are useful predictors because they have little overlap across real and fake articles.

```{r}
eratio <- buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio, fill=type, alpha=0.5))+
  geom_density()+
  labs(x="Exclamation Ratio in Body", fill="Type")

eration_title <- buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio_title, fill=type, alpha=0.5))+
  geom_density()+
  labs(x="Exclamation Ratio in Title", fill="Type")

gridExtra::grid.arrange(eratio, eration_title)
```

When looking at sentiment analysis, we see that the strength and type of primary sentiment are not very useful *except* in the case of surprise.  Surprise is only encountered in real articles, making it a good predictor of article type. As mentioned before, this fact changes if source is removed as a predictor. When that happens, sentiment becomes an important predictor.

```{r}
buzzfeed %>% 
  ggplot(aes(x=primary_sentiment_value, fill = type, alpha = 0.5))+
  geom_density()+
  facet_wrap(~primary_sentiment)+
  xlim(0,60)

buzzfeed %>% 
  ggplot(aes(x=type, fill = type))+
  geom_bar()+
  facet_wrap(~primary_sentiment)
```

Overall, source and author are two of the most indicative predictors. From the predictors we created, the exclamation ratio in both the text and the title and the sentiment analysis when source is removed play an important role in categorizing the article. The LASSO model we created from these predictors was extremely accurate, with or without source as a predictor. Furthermore, the specificty was extremely high. With source as a predictor, specificity is at 100%. Without source as a predictor, specificity is approximately 97%. Both values are extremely high, which is very important for this task. For categorizing an article as fake or real news, it is more important to have a high specificity because it is better to accurately know which articles are fake, rather than accidentally categorize an article as real news. Lastly, the sensitivity is also high, 77% and 90%, which is also very high for this model. Overall, as seen from our predictors and the sensitivity and specificty values, it is easier to categorize a fake article than a real article, yet both are fairly detectable given some of our best predictors.


\
\
\
\
\
\

## Part 4: Contributions

Both members evenly contributed to this project. We came up with the new predictors together and each implemented half of them. The rest was done together.
