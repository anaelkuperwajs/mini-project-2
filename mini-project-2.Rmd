---
title: "Mini-Project 2: Adventure 1"
author: Anael Kuperwajs Cohen, Juliet Kelson
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---


```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(stringr)
library(tidyverse)
library(quanteda)
library(syuzhet)
source("miniProjFunctions.R")
```


\
\



## Part 1: Process the data

```{r}
buzzfeed <- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv")

buzzfeed <- buzzfeed %>% 
  mutate(title = as.character(title),
         text = as.character(text),
         url = as.character(url))

'%!in%' <- function(x, y)!('%in%'(x,y))
```



### New predictors

Our new predictors will be:

[x] 1. Word count
[x] 2. Word count in title
[x] 3. Upper-case word count
[x] 4. Upper-case word count in title
[x] 5. ! ratio (to . ?)
[x] 6. ! ratio (to . ?) in title
[x] 7. Sentence Count
[x] 8. Syllables
[x] 9. % Unique words
[x] 10. Flesch–Kincaid grade level
[x] 11. Primary sentiment
[x] 12. Secondary sentiment
[x] 13. Strength of primary sentiment
[x] 14. Strength of secondary sentiment
[x] 15. Author Count
[x] 16. Upper-case word ratio in title


1. Word Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words = str_count(text, " ") + 1)
```

2. Word count in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words_title = str_count(title, " ") + 1)
```

3. Upper-case word count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words = str_count(text, "\\b[A-Z]{2,}\\b"))
```

4. Upper-case word count in title
16. Upper-case word ratio in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words_title = str_count(title, "\\b[A-Z]{2,}\\b"),
         upper_case_word_ratio_title = total_upper_case_words_title/total_words_title)
```

5. ! ratio (to . ?)
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio = exclamation_ratio(text))
```

6. ! ratio (to . ?) in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio_title = exclamation_ratio(title))
```

7. Sentence Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_sentences = nsentence(text))
```

8. Syllables
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_syllables = nsyllable(text))
```

9. % Unique words
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(unique_word_percent = pct_unique_words(text, total_words))
```

10. Flesch–Kincaid grade level
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(readability_score = flesch_reading_ease(total_words, total_sentences, total_syllables))
```

11. Primary sentiment
12. Secondary sentiment
13. Strength of primary sentiment
14. Strength of secondary sentiment
```{r}
buzzfeed <- buzzfeed %>% 
  rowwise() %>% 
  mutate(primary_sentiment = max_sentiment_type(text, 1),
         primary_sentiment_value = max_sentiment_value(text, 1),
         secondary_sentiment = max_sentiment_type(text, 2),
         secondary_sentiment_value = max_sentiment_value(text, 2)
         )
```

6.
```{r}
buzzfeed <- buzzfeed %>%
  mutate(authors = gsub("View All Posts,", "", authors)) %>%
  mutate(authors = gsub("View All Posts", "", authors)) %>%
  mutate(authors = gsub("Abc News,", "", authors)) %>%
  mutate(authors = gsub("Abc News", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter,", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent,", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent", "", authors)) %>%
  mutate(authors = gsub("Latest Posts,", "", authors)) %>%
  mutate(authors = gsub("Latest Posts", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer", "", authors)) %>%
  mutate(author_count = str_count(authors, ",") + 1) %>% 
  mutate(author_count = if_else(author_count == 0, 1, author_count)) %>% 
  mutate(authors = if_else(authors == "", "None", authors)) %>% 
  mutate(author_count = if_else(authors == "None", 0, author_count))
```


To demonstrate and summarize the definitions of your new predictors, discuss their measurements for one real sample article and one fake sample article of your choosing.

```{r}
buzzfeed[c(8,153),]
```

The two sources that will be presented in order to demonstrate our new predictors include "Jeb Bush to lecture at Harvard this fall" -- the real sample article -- and "Hillary's DEAD!?!? Brand New Theory Has Serious PROOF" --  the fake article. 
We first started by looking at predictors relating to word and punctuation counts in the text itself and the title. Our first predictors looked at the word count and upper-case word count in the text and the title. The total number of words for the title and the text were not greatly different between these two articles. The number of words in the title is exactly the same and the word count in the text is greater for the fake article, but only by 50 words. The total number of upper-case words in the text is also similar in both articles, one in the real article and two in the fake, however the difference in the number of upper-case words is more drastic in the title. There are zero upper-case words in the real title and two in the fake article, which is out of eight total words in the title. The next two predictors looked at the ratio of exclamation points to question marks and periods in the text and title. In the text, there are zero exclamation points in real article, yet about 47% in the fake article. Furthermore, in the title there are zero exclamation points in the real article but they make up 50% of the punctuation in the fake article. This shows that exclamation points are used way more freely in the fake article, rather than the real article.

There are of course drawbacks to text analysis. For example, let's look at sentiment analysis.  We will not always be able to accurately analyze the sentiment of text.  Computers and algorithms can't always detect sarcasm, understand references or allusions, and won't always correctly analyze basic sentiments such as `positive` or `negative`.
Additionally, writing algorithms to detect factors like writing style, words with multiple meanings, correct grammar/puntuation, and correctness of fact is often difficult, innacurate, or impossible.  All of that being said, text analysis is still a good and important research tool.


\
\
\
\
\
\



## Part 2: Analyze

When training our model, we don't want to include `text`, `title`, or `url` because they are unique to each article and therefore will not make good predictors.

```{r}
#Doesn't seem to be a good predictor
buzzfeed %>% 
  ggplot(aes(x=primary_sentiment_value, fill = type, alpha = 0.5))+
  geom_density()+
  facet_wrap(~primary_sentiment)+
  xlim(0,60)

buzzfeed %>% 
  ggplot(aes(x=secondary_sentiment_value, fill = type, alpha = 0.5))+
  geom_density()+
  facet_wrap(~secondary_sentiment)+
  xlim(0,60)

buzzfeed %>% 
  ggplot(aes(x=readability_score, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=unique_word_percent, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio_title, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=author_count, fill=type, alpha=0.5))+
  geom_density()
```


```{r}
# Pretty good 
set.seed(253)
lambda_grid <- 10^seq(-3, 1, length = 100)

# Perform LASSO
lasso_model <- train(
    type ~ .,
    data = buzzfeed %>% select(-text, -title, -url),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "MAE",
    na.action = na.omit
)

lasso_model$results %>% filter(lambda == lasso_model$bestTune$lambda)

coef(lasso_model$finalModel, lasso_model$bestTune$lambda)

```


```{r}
# GAM doesn't like the source or url because of the backslashes

gam_model <- train(
  type ~ .,
  data = buzzfeed %>% select(-text, -title, -url, -source),
  method = "gamLoess",
  tuneGrid = data.frame(span = seq(0.2, 0.6, 0.1), degree = 1),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "oneSE"),
  metric = "Accuracy",
  na.action = na.omit
)

plot(gam_model)
gam_model$resample
```


```{r}
#Not great and hard to use

# Set the seed
set.seed(253)

# Run the model
# using variables chosen from lasso
logistic_model <- train(
    type ~ .,
    data = buzzfeed %>% select(authors, source, exclamation_ratio_title, type, primary_sentiment, secondary_sentiment),
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 10),
    metric = "Accuracy",
    na.action = na.omit
)

# Summarize the model
logistic_model$results
```


\
\
\
\
\
\



## Part 3: Summarize




\
\
\
\
\
\

## Appendix






\
\
\
\
\
\

## Part 4: Contributions

