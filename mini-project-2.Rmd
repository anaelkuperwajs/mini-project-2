---
title: "Mini-Project 2: Adventure 1"
author: Anael Kuperwajs Cohen, Juliet Kelson
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---


```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(stringr)
library(tidyverse)
library(quanteda)
library(syuzhet)
source("miniProjFunctions.R")
```


\
\



## Part 1: Process the data

```{r}
buzzfeed <- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv")

buzzfeed <- buzzfeed %>% 
  mutate(title = as.character(title),
         text = as.character(text),
         url = as.character(url))

'%!in%' <- function(x, y)!('%in%'(x,y))
```



### New predictors

Our new predictors will be:

[x] 1. Word count
[x] 2. Word count in title
[x] 3. Upper-case word count
[x] 4. Upper-case word count in title
[x] 5. Upper-case word ratio in title
[x] 6. ! ratio (to . ?)
[x] 7. ! ratio (to . ?) in title
[x] 8. Sentence Count
[x] 9. Syllables
[x] 10. % Unique words
[x] 11. Flesch–Kincaid grade level
[x] 12. Author Count
[x] 13. Primary sentiment
[x] 14. Secondary sentiment
[x] 15. Strength of primary sentiment
[x] 16. Strength of secondary sentiment


1. Word Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words = str_count(text, " ") + 1)
```

2. Word count in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_words_title = str_count(title, " ") + 1)
```

3. Upper-case word count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words = str_count(text, "\\b[A-Z]{2,}\\b"))
```

4. Upper-case word count in title
5. Upper-case word ratio in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_upper_case_words_title = str_count(title, "\\b[A-Z]{2,}\\b"),
         upper_case_word_ratio_title = total_upper_case_words_title/total_words_title)
```

6. ! ratio (to . ?)
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio = exclamation_ratio(text))
```

7. ! ratio (to . ?) in title
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(exclamation_ratio_title = exclamation_ratio(title))
```

8. Sentence Count
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_sentences = nsentence(text))
```

9. Syllables
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(total_syllables = nsyllable(text))
```

10. % Unique words
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(unique_word_percent = pct_unique_words(text, total_words))
```

11. Flesch–Kincaid grade level
```{r}
buzzfeed <- buzzfeed %>% 
  mutate(readability_score = flesch_reading_ease(total_words, total_sentences, total_syllables))
```

12. Author Count
```{r}
buzzfeed <- buzzfeed %>%
  mutate(authors = gsub("View All Posts,", "", authors)) %>%
  mutate(authors = gsub("View All Posts", "", authors)) %>%
  mutate(authors = gsub("Abc News,", "", authors)) %>%
  mutate(authors = gsub("Abc News", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter,", "", authors)) %>%
  mutate(authors = gsub("Cnn National Politics Reporter", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent,", "", authors)) %>%
  mutate(authors = gsub("Cnn Pentagon Correspondent", "", authors)) %>%
  mutate(authors = gsub("Latest Posts,", "", authors)) %>%
  mutate(authors = gsub("Latest Posts", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn White House Producer", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer,", "", authors)) %>%
  mutate(authors = gsub("Cnn Senior Congressional Producer", "", authors)) %>%
  mutate(author_count = str_count(authors, ",") + 1) %>% 
  mutate(author_count = if_else(author_count == 0, 1, author_count)) %>% 
  mutate(authors = if_else(authors == "", "None", authors)) %>% 
  mutate(author_count = if_else(authors == "None", 0, author_count))
```

13. Primary sentiment
14. Secondary sentiment
15. Strength of primary sentiment
16. Strength of secondary sentiment
```{r}
buzzfeed <- buzzfeed %>% 
  rowwise() %>% 
  mutate(primary_sentiment = max_sentiment_type(text, 1),
         primary_sentiment_value = max_sentiment_value(text, 1),
         secondary_sentiment = max_sentiment_type(text, 2),
         secondary_sentiment_value = max_sentiment_value(text, 2)
         )
```


To demonstrate and summarize the definitions of your new predictors, discuss their measurements for one real sample article and one fake sample article of your choosing.

```{r}
buzzfeed[c(8,153),]
```

The two sources that will be presented in order to demonstrate our new predictors include "Jeb Bush to lecture at Harvard this fall" -- the real sample article -- and "Hillary's DEAD!?!? Brand New Theory Has Serious PROOF" --  the fake article. 
We first started by looking at predictors relating to word and punctuation counts in the text itself and the title. Our first predictors looked at the word count and upper-case word count in the text and the title. The total number of words for the title and the text were not significantly different between these two articles. The number of words in the title is exactly the same and the word count in the text is greater for the fake article, but only by 50 words. The total number of upper-case words in the text is also similar in both articles, one in the real article and two in the fake, however the difference in the number of upper-case words is more drastic in the title. There are zero upper-case words in the real title and two in the fake article, which is out of eight total words in the title. Two out of eight words is 25%, as seen in the ratio predictor for upper-case words in the title. The next two predictors looked at the ratio of exclamation points to question marks and periods in the text and title. In the text, there are zero exclamation points in real article, yet about 47% in the fake article. Furthermore, in the title there are zero exclamation points in the real article but they make up 50% of the punctuation in the fake article. This shows that exclamation points are used way more freely in the fake article, rather than the real article. Just like word count, we also looked at sentence and syllable count. These two measurements were fairly insignificant as the numbers were similar in both sentences. It was slightly surprising, however, that there were more sentences and syllables in the fake news article than the real one. The next two predictors analyzed the words that were used in the text. The first is the percent of unique words, which was much higher for the real article, 112.75% vs 87.10%. The second is a readability score, where the higher the percent the easier the text is to read because it is meant for a younger audience. The real article has a readability score of 23.34, which matches with a school reading level of a college graduate. On the other hand, the fake article has a score of 48.71, which aligns with a school reading level of a college student. While this may not seem like a huge difference, the idea is that the structure of the article is much more complex the lower the score. Fake articles often don't have that same complexity. The next predictor is about the author count, which is equal in this case between the two articles with one author. It does not make a big difference in this comparison. Finally, the last four predictors are about sentiment analysis. We looked at the primary and secondary sentiment in the text and the strength of that sentiment. The sentiments for the real article are trust and positive, with scores in the 30s, while the fake article has the sentiments negative and positive with low scores of 16 each. The fact that the primary and secondary sentiments conflict with each other greatly is something to be concerned about when thinking of the validity of an article.

Similar to any analytical method, text analysis has it's drawbacks. One drawback, for example, is seen clearly through the sentiment analysis predictors. We will not always be able to accurately analyze the sentiment of text. Computers and algorithms can't always detect sarcasm, understand references or allusions, and won't always correctly analyze basic sentiments such as `positive` or `negative`. Additionally, writing algorithms to detect factors like writing style, words with multiple meanings, correct grammar/puntuation, and correctness of fact is often difficult, innacurate, or impossible.  All of that being said, text analysis is still a beneficial and important research tool.


\
\
\
\
\
\



## Part 2: Analyze

When training our model, we don't want to include `text`, `title`, or `url` because they are unique to each article and therefore will not make good predictors.

```{r}
#Doesn't seem to be a good predictor
buzzfeed %>% 
  ggplot(aes(x=primary_sentiment_value, fill = type, alpha = 0.5))+
  geom_density()+
  facet_wrap(~primary_sentiment)+
  xlim(0,60)

buzzfeed %>% 
  ggplot(aes(x=secondary_sentiment_value, fill = type, alpha = 0.5))+
  geom_density()+
  facet_wrap(~secondary_sentiment)+
  xlim(0,60)

buzzfeed %>% 
  ggplot(aes(x=readability_score, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=unique_word_percent, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=exclamation_ratio_title, fill=type, alpha=0.5))+
  geom_density()

buzzfeed %>% 
  ggplot(aes(x=author_count, fill=type, alpha=0.5))+
  geom_density()
```


```{r}
# Pretty good 
set.seed(253)
lambda_grid <- 10^seq(-3, 1, length = 100)

# Perform LASSO
lasso_model <- train(
    type ~ .,
    data = buzzfeed %>% select(-text, -title, -url),
    method = "glmnet",
    tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
    trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
    metric = "MAE",
    na.action = na.omit
)

lasso_model$results %>% filter(lambda == lasso_model$bestTune$lambda)

coef(lasso_model$finalModel, lasso_model$bestTune$lambda)

```


```{r}
# GAM doesn't like the source or url because of the backslashes

gam_model <- train(
  type ~ .,
  data = buzzfeed %>% select(-text, -title, -url, -source),
  method = "gamLoess",
  tuneGrid = data.frame(span = seq(0.2, 0.6, 0.1), degree = 1),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "oneSE"),
  metric = "Accuracy",
  na.action = na.omit
)

plot(gam_model)
gam_model$resample
```


```{r}
#Not great and hard to use

# Set the seed
set.seed(253)

# Run the model
# using variables chosen from lasso
logistic_model <- train(
    type ~ .,
    data = buzzfeed %>% select(authors, source, exclamation_ratio_title, type, primary_sentiment, secondary_sentiment),
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 10),
    metric = "Accuracy",
    na.action = na.omit
)

# Summarize the model
logistic_model$results
```


\
\
\
\
\
\



## Part 3: Summarize




\
\
\
\
\
\

## Appendix






\
\
\
\
\
\

## Part 4: Contributions

